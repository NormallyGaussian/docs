---
title: Parallel Extract
---

>[Parallel](https://platform.parallel.ai/) is a real-time web search and content extraction platform designed specifically for LLMs and AI applications.

The **ParallelExtractTool** retrieves clean, structured content from web pages via Parallel's Extract API. It automatically removes ads, navigation menus, and boilerplate content, returning clean markdown-formatted text optimized for LLM consumption.

Parallel's content extraction provides reliable access to web content without the overhead of traditional scraping. The platform offers unique features like focused extraction (target specific information using natural language), batch processing for multiple URLs, configurable excerpt lengths, and robust error handling for failed extractions.

This notebook goes over how to use Parallel Extract with LangChain.

## Setup

### Installation

Install the LangChain Parallel integration package:

```python
pip install -qU langchain-parallel
```

### Credentials

You'll need a Parallel API key to use this integration. Get started by [signing up here](https://platform.parallel.ai).

```python
import getpass
import os

if not os.environ.get("PARALLEL_API_KEY"):
    os.environ["PARALLEL_API_KEY"] = getpass.getpass("Parallel API key:\n")
```

## Using ParallelExtractTool

ParallelExtractTool is a tool that can be used with LangChain agents to extract clean content from web pages. It provides a structured interface for content extraction with markdown-formatted output:

```python
from langchain_parallel import ParallelExtractTool

# Initialize the ParallelExtractTool
extract_tool = ParallelExtractTool(api_key=os.environ["PARALLEL_API_KEY"])

# Extract content from a single URL
result = extract_tool.invoke(
    {"urls": ["https://en.wikipedia.org/wiki/Artificial_intelligence"]}
)

print(f"Extracted {len(result)} result(s)")
print(f"Title: {result[0]['title']}")
print(f"URL: {result[0]['url']}")
print(f"Content length: {len(result[0]['content'])} characters")
print(f"Content preview: {result[0]['content'][:200]}...")
```

The tool accepts a list of URLs and returns structured results including:
- `url`: The source URL
- `title`: Page title
- `content`: Clean, markdown-formatted content
- `publish_date`: Publication date (when available)
- `error_type`: Error description for failed extractions (e.g., "http_error")

### Batch Extraction

Extract content from multiple URLs in a single request:

```python
# Extract from multiple URLs
result = extract_tool.invoke(
    {
        "urls": [
            "https://en.wikipedia.org/wiki/Machine_learning",
            "https://en.wikipedia.org/wiki/Deep_learning",
            "https://en.wikipedia.org/wiki/Natural_language_processing",
        ]
    }
)

# Process results
print(f"Extracted {len(result)} results")
for i, item in enumerate(result, 1):
    print(f"\n{i}. {item['title']}")
    print(f"   URL: {item['url']}")
    print(f"   Content length: {len(item['content'])} characters")
```

## Advanced Features for ParallelExtractTool

### Focused Extraction

Use the `search_objective` parameter to focus extraction on specific information:

```python
# Extract only content relevant to a specific topic
result = extract_tool.invoke(
    {
        "urls": ["https://en.wikipedia.org/wiki/Artificial_intelligence"],
        "search_objective": "What are the main applications and ethical concerns of AI?",
        "excerpts": {"max_chars_per_result": 2000},
        "full_content": False,
    }
)

print(f"Extracted focused excerpts: {len(result[0].get('excerpts', []))} excerpts")
print(f"Content preview: {result[0]['content'][:200]}...")
```

This is useful when you need specific information from large pages without processing the entire document.

### Fetch Policies and Caching

Configure caching behavior and timeouts with the `fetch_policy` parameter:

```python
# Extract with fetch policy for fresh content
result = extract_tool.invoke(
    {
        "urls": ["https://en.wikipedia.org/wiki/Quantum_computing"],
        "fetch_policy": {
            "max_age_seconds": 86400,  # 1 day cache
            "timeout_seconds": 60,
            "disable_cache_fallback": False,
        },
        "full_content": {"max_chars_per_result": 5000},
    }
)

print(f"Content length: {len(result[0]['content'])} characters")
```

**Fetch policy parameters:**
- `max_age_seconds`: Cache duration - how long to use cached content
- `timeout_seconds`: Request timeout - maximum time to wait for page load
- `disable_cache_fallback`

### Key Parameters

- **urls**: List of URLs to extract content from (required)
- **search_objective**: Natural language description to focus extraction on specific information (optional)
- **excerpts**: Configuration for excerpt length:
  - `max_chars_per_result`: Maximum characters per result for compressed, contextual excerpts (same as in ParallelWebSearchTool)
- **full_content**: Boolean to get full content vs. excerpts (default: True)
- **fetch_policy**: Cache and timeout settings (optional):
  - `max_age_seconds`: Cache duration - how long to use cached content
  - `timeout_seconds`: Request timeout - maximum time to wait for page load

## Async Support

ParallelExtractTool supports asynchronous operations for better performance in async applications:

```python
import asyncio

async def extract_async():
    return await tool.ainvoke(
        {
            "urls": [
                "https://en.wikipedia.org/wiki/Python_(programming_language)",
                "https://en.wikipedia.org/wiki/JavaScript",
            ]
        }
    )

# Run async extraction
result = asyncio.run(extract_async())
print(f"Extracted {len(result)} results asynchronously")
```

## Error Handling

The tool gracefully handles URLs that fail to extract, including them in results with error information:

```python
# Advanced extraction with all options
result = extract_tool.invoke(
    {
        "urls": [
            "https://en.wikipedia.org/wiki/Artificial_intelligence",
            "https://this-domain-does-not-exist-12345.com/",
        ]
    }
)

for item in result:
    if "error_type" in item:
        print(f"Failed: {item['url']}")
        print(f"Error: {item['content']}")
    else:
        print(f"Success: {item['url']}")
        print(f"Extracted {len(item['content'])} characters")
```

**Common error scenarios:**
- **Missing API key**: Ensure `PARALLEL_API_KEY` environment variable is set
- **Invalid URLs**: Tool returns `error_type` in result for unreachable pages
- **Timeout errors**: Increase `request_timeout` in fetch_policy for slow-loading pages

---

## Related

- [Parallel provider documentation](/oss/integrations/providers/parallel) - Overview of ChatParallelWeb chat model and ParallelWebSearchTool
- [Parallel Search tool documentation](/oss/integrations/tools/parallel_search) - Real-time web search with LLM-optimized results

## API reference

For detailed documentation of all Parallel API features and configurations, visit the [Parallel Extract API documentation](https://docs.parallel.ai/api-reference/extract-beta/extract).
